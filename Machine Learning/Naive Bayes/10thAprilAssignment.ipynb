{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f42c533-a9ff-4904-8ae4-d0d171e351a3",
   "metadata": {},
   "source": [
    "**Q1.** A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Apologies for the confusion in the previous response. Let's solve the problem in brief:\n",
    "\n",
    "Given information:\n",
    "- 70% of the employees use the company's health insurance plan (P(A) = 0.70).\n",
    "- 40% of the employees who use the plan are smokers (P(B|A) = 0.40).\n",
    "\n",
    "We are asked to find the probability that an employee is a smoker given that he/she uses the health insurance plan (P(B|A)).\n",
    "\n",
    "Using Bayes' theorem, the formula for finding $P(B|A)$ is:\n",
    "\n",
    "$ P(B|A) = \\frac{P(A|B) \\cdot P(B)}{P(A)} $\n",
    "\n",
    "Since we don't have direct information on $P(B)$ or $P(A|B)$, we cannot compute $P(B|A)$ precisely without making additional assumptions or having more data.\n",
    "\n",
    "However, if we assume that the probability of being a smoker is the same whether an employee uses the health insurance plan or not (i.e., $P(B|A) = P(B|\\neg A))$, we can estimate $P(B|A)$ as follows:\n",
    "\n",
    "$ P(B|A) = \\frac{P(B) \\cdot P(A|B)}{P(A)} $\n",
    "\n",
    "Using the assumption that \\(P(B) = P(B|A)\\), we can simplify further:\n",
    "\n",
    "$ P(B|A) = \\frac{P(B) \\cdot P(A|B)}{P(A)} = \\frac{0.40 \\cdot 0.70}{0.70} = 0.40 $\n",
    "\n",
    "Under this assumption, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.40 or 40%. However, please note that this value depends on the assumption we made about $P(B) = P(B|A)$ and may not represent the true probability without additional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03befb92-7a64-4eb7-84f3-89bcb30f24e8",
   "metadata": {},
   "source": [
    "**Q2.** What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of data they are designed to handle and the assumptions they make about the distribution of features.\n",
    "\n",
    "1. **Bernoulli Naive Bayes**:\n",
    "   - Data Type: Bernoulli Naive Bayes is suitable for binary data, where features are represented as 0s and 1s (e.g., presence or absence of a particular attribute).\n",
    "   - Assumption: It assumes that each feature is conditionally independent of others given the class label. It models the presence or absence of each feature independently for each class.\n",
    "   - Use Case: Bernoulli Naive Bayes is commonly used in text classification tasks, document classification, and problems where the input data is binary or can be transformed into binary features.\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - Data Type: Multinomial Naive Bayes is suitable for discrete data, where features represent counts or frequencies (e.g., word counts in a document).\n",
    "   - Assumption: It assumes that each feature follows a multinomial distribution. It models the likelihood of observing each feature value (count or frequency) for each class.\n",
    "   - Use Case: Multinomial Naive Bayes is frequently used in text classification tasks, such as spam filtering, sentiment analysis, and document categorization, where the input features are word frequencies or other discrete counts.\n",
    "\n",
    "In summary, the key distinction between Bernoulli Naive Bayes and Multinomial Naive Bayes is the type of data they can handle and the distributional assumptions they make. If the input features are binary (0/1) and represent the presence or absence of attributes, Bernoulli Naive Bayes is a suitable choice. On the other hand, if the input features are discrete counts or frequencies, Multinomial Naive Bayes is more appropriate. Both classifiers are popular choices in text-related tasks due to their simplicity, efficiency, and ability to handle high-dimensional data like word frequencies in documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae32a7df-02f6-4a3a-b7ab-37e4481dce49",
   "metadata": {},
   "source": [
    "**Q3.** How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Bernoulli Naive Bayes handles missing values in a straightforward manner. When dealing with missing values in the input data, Bernoulli Naive Bayes simply ignores the missing values during the calculation of probabilities.\n",
    "\n",
    "Here's how Bernoulli Naive Bayes handles missing values:\n",
    "\n",
    "1. **Training Phase**:\n",
    "   - During the training phase, Bernoulli Naive Bayes estimates the probabilities of the presence or absence of each feature for each class based on the available training data.\n",
    "   - If a particular feature is missing for a specific instance in the training data, that instance is simply excluded from the calculation of probabilities for that feature and class.\n",
    "   - In other words, missing values are treated as if the corresponding features are not present, and the model proceeds with the available features.\n",
    "\n",
    "2. **Testing Phase**:\n",
    "   - During the testing phase, when predicting the class of a new instance that contains missing values for some features, the model also ignores those missing values.\n",
    "   - The model makes predictions based on the available features, using the estimated probabilities from the training phase.\n",
    "   - If a feature is missing in the new instance, the model assumes that the corresponding feature is not present (0) when calculating the probabilities for that class.\n",
    "\n",
    "By ignoring missing values and treating them as if the corresponding features are not present, Bernoulli Naive Bayes ensures that predictions can still be made even when some feature values are not available for a given instance. However, it's important to note that handling missing values in this way may introduce biases in the model, particularly if the missing values are not missing completely at random (MCAR) or are informative about the class label. In such cases, more sophisticated techniques, such as imputation or advanced probabilistic models, may be necessary to handle missing data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fcee5e-2626-4083-9b49-28993a39b35d",
   "metadata": {},
   "source": [
    "**Q4.** Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes that the features follow a Gaussian (normal) distribution within each class. It is commonly used for continuous numerical data.\n",
    "\n",
    "In the case of multi-class classification, where there are more than two classes, Gaussian Naive Bayes can be extended to handle multiple classes by applying the \"one-vs-all\" (OvA) or \"one-vs-one\" (OvO) strategy.\n",
    "\n",
    "1. **One-vs-All (OvA)**:\n",
    "   - In the OvA strategy, a separate binary Gaussian Naive Bayes classifier is trained for each class. Each classifier distinguishes between instances of one class (positive class) and instances of all the other classes (negative classes).\n",
    "   - During prediction, the instance is classified by all the binary classifiers, and the class with the highest probability is chosen as the final prediction.\n",
    "\n",
    "2. **One-vs-One (OvO)**:\n",
    "   - In the OvO strategy, a separate binary Gaussian Naive Bayes classifier is trained for every pair of classes. For N classes, there are N * (N-1) / 2 binary classifiers.\n",
    "   - During prediction, each binary classifier votes for its corresponding class. The class with the most votes is chosen as the final prediction.\n",
    "\n",
    "Both the OvA and OvO strategies are popular approaches for extending binary classifiers like Gaussian Naive Bayes to handle multi-class classification problems. They allow us to use binary classifiers to perform multi-class classification efficiently. The choice between OvA and OvO may depend on the dataset size, computational resources, and specific characteristics of the problem.\n",
    "\n",
    "In conclusion, Gaussian Naive Bayes can be adapted for multi-class classification by using either the one-vs-all or one-vs-one strategy, allowing it to handle problems with more than two classes effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dec5f58-63b3-4587-a00f-b1a57b83a541",
   "metadata": {},
   "source": [
    "**Q5.** Assignment:\n",
    "Data preparation:\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "\n",
    "\n",
    "Implementation:\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "\n",
    "Results:\n",
    "\n",
    "Report the following performance metrics for each classifier:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "\n",
    "Discussion:\n",
    "\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33644d3e-3f3f-4c8e-93f6-5a0a74ec683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11c18ca7-a495-4d1e-a526-e39e4ad32337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the UCI Machine Learning Repository\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "data = pd.read_csv(url, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89bb9d2d-04b7-49e7-a356-d61c1271da89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07e26f97-4dda-42f6-b76e-b5ea5a725755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target (y)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e41574e6-9201-4146-8cff-d131101af9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the three Naive Bayes classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c99a53ce-00b9-4204-9461-a934ee312708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 10-fold cross-validation and get performance metrics for each classifier\n",
    "classifiers = [bernoulli_nb, multinomial_nb, gaussian_nb]\n",
    "clf_names = [\"Bernoulli Naive Bayes\", \"Multinomial Naive Bayes\", \"Gaussian Naive Bayes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e15b495-e219-400b-a5ac-cdef7a9e28e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839\n",
      "Precision: 0.8870\n",
      "Recall: 0.8152\n",
      "F1 Score: 0.8481\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863\n",
      "Precision: 0.7393\n",
      "Recall: 0.7215\n",
      "F1 Score: 0.7283\n",
      "\n",
      "Results for Gaussian Naive Bayes:\n",
      "Accuracy: 0.8218\n",
      "Precision: 0.7104\n",
      "Recall: 0.9570\n",
      "F1 Score: 0.8131\n"
     ]
    }
   ],
   "source": [
    "for clf, clf_name in zip(classifiers, clf_names):\n",
    "    accuracy = cross_val_score(clf, X, y, cv=10, scoring='accuracy').mean()\n",
    "    precision = cross_val_score(clf, X, y, cv=10, scoring='precision').mean()\n",
    "    recall = cross_val_score(clf, X, y, cv=10, scoring='recall').mean()\n",
    "    f1_score = cross_val_score(clf, X, y, cv=10, scoring='f1').mean()\n",
    "\n",
    "    print(f\"\\nResults for {clf_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7984a-0fa1-47e8-a481-d7bf7f102919",
   "metadata": {},
   "source": [
    "THe accuracy of Bernoulli Naive Bayes is better than other two  models. One limitation of Naive Bayes is its strong independence assumption, which might not hold in real-world data. Also, it can struggle when dealing with features that are highly correlated or when the class distributions are imbalanced. Nevertheless, Naive Bayes can be a fast and effective choice for text classification and other similar tasks when the independence assumption is approximately met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea241384-b09d-4b28-a070-cefa1a44bd78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
